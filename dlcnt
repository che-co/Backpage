#!/bin/bash 
#  This script is used to download and extract page content
#+ scraped using scraper.sh
#  Usage: dlcnt -t 

E_WRONG_ARGS=85
E_EMPTY_PATH=84
E_EMPTY_CONTENT=44

source ./.scraper.conf

#  Set options
TOR=true
DUP=false

while :; do
  case $1 in
    -d | --duplicate )
      DUP=true ;;
    -n | --no-tor ) 
      TOR=false ;;
    -t | --type )
      if [ ! -z "$2" ]; then
        TYPE="$2"
        shift
      else
        echo "`basename "$0"`: \"--type\" requires an argument and has no default." >&2
        exit $E_WRONG_ARGS
      fi ;;
    --type=?*)
      TYPE=${1#*=} ;;
    --) shift ; break ;;
    *) break ;;
  esac
  shift
done

if [ $TOR = true ]; then
  TOR_OPT=torify
fi

if [ -z "$TYPE" ]; then
  TYPE="*"
fi

if [ $# -ge 1 ]; then
  LIST=$@
  LIST=${LIST// /\\|}
  THESE=`ls -d ./URLs/*/*/$TYPE/* | grep -v "$LIST" | grep -v downloaded`
else
  THESE=`ls -d ./URLs/*/*/$TYPE/* | grep -v downloaded`
fi

for UPATH in $THESE; do
  if [ -e "${UPATH%/*}"/downloaded ]; then
    grep -q "$UPATH" "${UPATH%/*}"/downloaded
    if [ $? -eq 0 ]; then
      continue
    fi
  fi 
  CPATH="./content/${UPATH#./URLs/}"
  CPATH="${CPATH%/*}"
  GEOG="${UPATH#./URLs/}"
  GEOG="${GEOG%%/*}"

  if [ ! -d "$CPATH" ]; then
    mkdir -p "$CPATH"
  fi

  while read URL; do
    $TOR_OPT curl $CURL_OPTS "http://"$GEOG".backpage.com"$URL"" 2>> ./curl_err > "$CPATH"/.ad.txt
    WRITEBODY=0
	
    while read LINE; do
      grep -q "</div>" <(echo "$LINE")
      if [ $? -eq 0 -a "$WRITEBODY" -eq 1 ]; then
        break
      fi
      if [ "$WRITEBODY" -eq 1 ]; then
        BODY="${BODY}""${LINE}"$'\n'
      fi
      grep -q "<div class=\"postingBody\">" <(echo "$LINE")
      if [ $? -eq 0 ]; then
        WRITEBODY=1
      fi
    done < "$CPATH"/.ad.txt
    BODY=`./clnvar "${BODY//<br>/ }"`
  
    DATETIME=`grep -o "[[:upper:]][[:lower:]]\+, [[:upper:]][[:lower:]]\+ [0-9]\+, [0-9]\{4\} [0-9]\+:[0-9]\{2\} [APM]*" "$CPATH"/.ad.txt`
    #  Extract "poster's" age
    AGE=`grep -oP "(?<=Poster's age: )[0-9]*" "$CPATH"/.ad.txt`

    LOCATION=`grep -A1 "Location:" "$CPATH"/.ad.txt | tail -1`
    LOCATION=`./clnvar "$LOCATION"`

    ID=`grep -oP '(?<=Post ID: )[0-9]*' "$CPATH"/.ad.txt`
		
    if [ -z "$BODY" ]; then
      echo "`basename "$0"`: the minumum expected content (BODY) was not found for posting \"http://"$GEOG".backpage.com"$URL"\"" >&2
      echo http://"$GEOG".backpage.com"$URL" >> ./unsucessful_urls
      continue
    fi

    if [ -z "$DATETIME" ]; then
      echo "`basename "$0"`: the minumum expected content (DATETIME) was not found for posting \"http://"$GEOG".backpage.com"$URL"\"" >&2
      echo http://"$GEOG".backpage.com"$URL" >> ./unsuccessful_urls
      continue
    fi

    if [ -z "$AGE" ]; then
      echo "`basename "$0"`: the minumum expected content for posting AGE was not found for \"http://"$GEOG".backpage.com"$URL"\"" >&2
      echo http://"$GEOG".backpage.com"$URL" >> ./unsuccessful_urls
      continue
    fi

    if [ -z "$ID" ]; then
      echo "`basename "$0"`: the minumum expected content for posting ID was not found for \"http://"$GEOG".backpage.com"$URL"\"" >&2
      echo http://"$GEOG".backpage.com"$URL" >> ./unsucessful_urls
      continue
    fi

    OTHERADS=`grep -v "http://"$GEOG".backpage.com/"$URL"" <(grep -o "http://[a-z]\+\./backpage\.com/[A-Za-z]\+/[a-z\-]\+/[0-9]\+" "$CPATH"/.ad.txt)`

    if [ ! -d "$CPATH"/"$ID" ]; then
      mkdir -p "$CPATH"/"$ID"/imgs
      CCPATH="$CPATH"/"$ID"
    else
      if [ $DUP = true ]; then
        CCPATH="$CPATH"/"$ID"\(`ls "$CPATH" | grep -c "$ID"`\)
        mkdir -p "$CPATH"/"$ID"\(`ls "$CPATH" | grep -c "$ID"`\)/imgs
        echo "$CPATH" ID: "$ID" already exists and may be duplcated
      else
        break
      fi
    fi

    IPATH="$CCPATH"/imgs

    echo writing contents for "$ID" to "$CCPATH"
    echo "$DATETIME" > "$CCPATH"/datetime.txt
    echo "$BODY"	 > "$CCPATH"/body.txt
    echo "$AGE"	 > "$CCPATH"/age.txt
    echo "$LOCATION" > "$CCPATH"/location.txt
    echo http://"$GEOG".backpage.com"$URL" > "$CCPATH"/url.txt
    if [ ! -z "$OTHERADS" ]; then
      echo "$OTHERADS" > "$CCPATH"/otheradurls.txt
    fi
    mv "$CPATH"/.ad.txt "$CCPATH"/ad.txt
    IMGURL=`paste -sd, <(grep -Po '(?<=(img src="))[^ ]*(?=")' "$CCPATH"/ad.txt)`
    if [ ! -z "$IMGURL" ]; then
      cd "$IPATH"
      echo writing pictures for "$ID" to "$IPATH"
      curl -Os "{"$IMGURL"}"
      cd - > /dev/null
    fi
  
    echo "\""$DATETIME"\",\""$BODY"\","$AGE",\""$LOCATION"\","$ID","$?",http://"$GEOG".backpage.com"$URL"" >> "${CPATH%/*}"/posting_content_raw.csv

    BODY=;DATETIME=;AGE=;LOCATION=;ID=;OTHERADS=;IMGURL=
  done < "$UPATH"
  echo "$UPATH" >> "${UPATH%/*}"/downloaded 
done

if [ -e ./.dontstop ]; then
  ./"$0"
else
exit $?
fi
