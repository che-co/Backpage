#!/bin/bash -x 
#  Author: Sergio "Checo" Gonzales
#  Date: 28-05-2016
#  This can be used to scrape the backpage.com

DATE=`date +%d-%m-%Y`
CATEGORY="$1"
NEWNAME="$CATEGORY"_"$2"_"$DATE"
NEWPOSTING="$NEWNAME".txt

#  Test for valid params and necessary directories.
#  If directories do not exist, create them.

if [ -z "$1" ]; then
 echo "Error: Posting category or categories not given without defulat"
 echo "Usage: $0 [category,category,...] [geography] [geography] ..."
 exit 1
fi

if [ -z "$2" ]; then
 echo "Error: Geography or georgraphies not given without default"
 echo "Usage: $0 [category,category,...] [geography] [geography] ..."
 exit 1 
fi

if [ ! -d ./URLS ]; then
 echo './URLS did not previously exist'
 mkdir ./URLS
fi

#  Select geographies.. not sure how to implement this yet

#  Scrape topics
until [ -z "$2" ]; do

 NEWNAME="$2"_"$CATEGORY"_"$DATE"

##  May be useful if running twice in one day...
# if [ `ls ./URLS | grep -c "$NEWNAME"` -eq 0 ]; then
#  NEWPOSTING="$NEWNAME".txt
# else
#   NEWPOSTING="$NEWNAME"\(`ls ./URLS | grep -c "$NEWNAME"`\).txt
# fi
##

#  Download URLS and check remove URLS previously downloaded
#  Still need to implement for subsequent pages

 curl -s "$2".backpage.com/{"$CATEGORY",:} | grep -o -e"http://[a-z]*.backpage.com/"{"$CATEGORY",:}"/[a-z\-]*/[0-9]*" | uniq > ./URLS/"$NEWNAME"_temp.txt

 if [ `ls ./URLS/"$2"* | grep -e{"$CATEGORY",:}` -le 1 ]; then 
   sort -o ./URLS/"$NEWNAME"_temp.txt ./URLS/"$NEWNAME"_temp.txt
   mv ./URLS/"$NEWNAME"_temp.txt ./URLS/"$NEWPOSTING"
 else
   sort -u `ls ./URLS/"$2"* | grep -e{"$CATEGORY",:}` > ./URLS/"$NEWPOSTING"
   rm ./URLS/"$NEWNAME"_temp.txt
 fi
   echo "$2" completed
   shift
done

#  Record ads that are paid advertisment and remove duplicates
#+ from other URL lists



#  Download and organize content



exit $?
