#!/bin/bash -x 

#  To-do: 
#  (1) Problem with Error codes... 5XX, 401-404, maybe more
#  (2) Stop downloading redundant pages
#  (3) Parallelize sponsored and non-sponsored scripts
#+     -needs to be integreated with dlcnt.sh
#  (4) Add option to contiue past duplicated URLS

#  Author: Sergio "Checo" Gonzales
#  Date: 07-17-2016
#  This was designed to scrape the escort ads of backpage.com

#  Some configuation variables and error codes
TIME_OUT=6
SLEEP=5
E_WRONG_ARG=85
OPTS="$1"

#  Usage errors
if [ -z "$OPTS" ] || [ "${OPTS:2:1}" != "u" -a ! -z "${OPTS:2:1}" ]; then
  echo "`basename "$0"`: Error: mandiroty options incorrectly specified" >&2
  echo "Usage: $0 -[nsu] [category,category,...] [geography] [geography] ..."
  exit $E_WRONG_ARGS 
fi
if [ -z "$2" ]; then
  echo "`basename "$0"`:Error: Posting category or categories not given without defulat" >&2
  echo "Usage: $0 -[nsu] [category,category,...] [geography] [geography] ..."
  exit $E_WRONG_ARGS 
fi

if [ -z "$3" ]; then
  echo "`basename "$0"`:Error: Geography or georgraphies not given without default" >&2
  echo "Usage: $0 -[nsu] [category,category,...] [geography] [geography] ..."
  exit $E_WRONG_ARGS 
fi

#  Option specifications
if [ "${OPTS:1:1}" = "n" ]; then
  export TYPE="non-sponsored"
  export GREP_OPT="-v"
elif [ "${OPTS:1:1}" = "s"]; then
  export TYPE="sponsored"
  export GREP_OPT=
else
  echo "`basename "$0"`: unsupporeted specification of options"
  echo "Usage: $0 -[nsu] [category,category,...] [geography] [geography] ..."
  exit $E_WRONG_ARG
fi 

#  Set some more variables
CATEGORY="$2,"
DATE=`date +%d%m%Y_%H%M%S`

#  URL grep function--not sure if I want to try this yet
#grep-urls () {
#
#}

#  Scrape topics
until [ -z "$3" ]; do
  GEOG="$3"
  for X in `eval echo {$CATEGORY}`; do
    export UPATH=./URLs/"$GEOG"/"$X"/"$TYPE"
    export PPATH=./.pages/"$GEOG"/"$X"
    export CPATH=./content/"$GEOG"/"$X"/"$TYPE"
    PAGE=1
    FNAME=""$GEOG"_"$X"_"$DATE""

    if [ ! -d "$UPATH" ]; then
      mkdir -p "$UPATH"
    fi   
    if [ ! -d "$PPATH" ]; then
      mkdir -p "$PPATH"
    fi
    if [ ! -d "$CPATH" ]; then
      mkdir -p "$CPATH"
    fi
    if [ ! -e ./content/"$GEOG"/"$X"/posting_content_raw.csv -o ! -s ./content/"$GEOG"/"$X"/posting_content_raw.csv ]; then
      echo DATTIME,BODY,AGE,LOCATION,ID,ADTYPE,URL > ./content/"$GEOG"/"$X"/posting_content_raw.csv
    fi

    until [ "$PAGE" -gt 444 ]; do
      FAILURES=0
      TOR_OPT=
      until [ -a "$UPATH"/.urls_"$DATE" ] && [ ! -s "$UPATH"/.urls_"$DATE" ] || [ $FAILURES -eq $TIME_OUT ]; do

        echo downloading "$TYPE" URLs from: ""$GEOG".backpage.com/"$X"/&page="$PAGE""
        $TOR_OPT curl -s ""$GEOG".backpage.com/"$X"/&page="$PAGE"" > "$PPATH"/"$FNAME"_pg"$PAGE"
        grep -q  "<div><b>No matches found.</b><br><br></div>" "$PPATH"/"$FNAME"_pg"$PAGE"
        if [ $? -eq 0 ]; then
          break
        fi
     
        grep $GREP_OPT "sponsorBoxPlusImages" "$PPATH"/"$FNAME"_pg"$PAGE" | grep -o /"$X"/[a-z\-]*/[0-9]* | sort | uniq > "$UPATH"/.urls_"$DATE"

        ERR=`grep -o "Error [0-9]*" "$PPATH"/"$FNAME"_pg"$PAGE"`
        if [ ! -z "$ERR" ]; then
          if [ "${OPTS:3:1}" = "u" ]; then
            echo "`basename "$0"`: HTTP request $ERR: trying again with sleep dealy" >&2
            sleep $SLEEP
          else
            echo "`basename "$0"`: HTTP request $ERR: tyring again with masked IP address" >&2
            TOR_OPT="torify" 
          fi
        else
          echo "`basename "$0"`: warning: unknown error; unable to find content" >&2
        fi
        ((FAILURES+=1))
      done
      if [ ! -s "$UPATH"/.urls_"$DATE" ]; then
        echo "`basename "$0"`: warning: no relevant $TYPE content found on "$GEOG".backpage.com/"$X"/&page="$PAGE"" >&2
        break 1 
      fi

      if [ `ls -A -1 "$UPATH"/ | wc -l` -le 1 ]; then 
        mv "$UPATH"/.urls_"$DATE" "$UPATH"/"$FNAME"
      else
        sort -u "$UPATH"/.urls_"$DATE" `ls -A -d  "$UPATH"/*` > "$UPATH"/._urls_"$DATE"
        comm -23 "$UPATH"/._urls_"$DATE" "$UPATH"/.urls_"$DATE" > "$UPATH"/"$FNAME" 
        rm "$UPATH"/.urls_"$DATE" "$UPATH"/._urls_"$DATE" 
        if [ -s "$UPATH"/"$FNAME" ]; then
          echo "no new unique urls found on page: "$GEOG".backpage.com/"$X"/&page="$PAGE""
          break
        fi 
      fi
			
      ((PAGE+=1))
    done 
    
#    ./dlcnt "NS" "$GEOG" "$X" "$FNAME"
#    if [ $? -ne 0 -a $? -ne 44 ]; then
#       exit $?
#    fi
#    ./dlcnt "S" "$GEOG" "$X" "$FNAME"
#    if [ $? -ne 0 -a $? -ne 44 ]; then
#       exit $?
#    fi 
  done
  shift
done

exit $?
