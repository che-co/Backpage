#!/bin/bash  
shopt -s extglob 

#  To-do:

#  Author: Sergio "Checo" Gonzales
#  Date: 07-28-2016
#  This can be used to scrape the backpage.com
CATEGORY="$1,"
DATE=`date +%d-%m-%Y`
#  Test for valid params and necessary directories.
#  If directories do not exist, create them.

clean_var() {
	local RESULT=$1
	RESULT="${RESULT//[$'\n\t\r\f']/}"
	RESULT="${RESULT#"${RESULT%%[![:space:]]*}"}"
	RESULT="${RESULT%"${RESULT##*[![:space:]]}"}"
	echo -n "$RESULT"
}
download_content () {	
	CPATH=""$1"CPATH"
	UPATH=""$1"UPATH"
	while read URL; do
		curl -s "http://"$2".backpage.com/"$URL"" > "$CPATH"/.ad.txt
		WRITEBODY=0
		
		#  Extract posting's body content
		while read LINE; do
			grep -q "</div>" <(echo "$LINE")
			if [ $? -eq 0 -a "$WRITEBODY" -eq 1 ]; then
				break
			fi
			if [ "$WRITEBODY" -eq 1 ]; then
				BODY="${BODY}""${LINE}"$'\n'
			fi
			grep -q "<div class=\"postingBody\">" <(echo "$LINE")
			if [ $? -eq 0 ]; then
				WRITEBODY=1
			fi
		done < "${!CPATH}"/.ad.txt
		BODY=`clean_var "${BODY//<br>/ }"`
		
		#  Extract date and time of posting
		DATETIME=`grep -o "[[:upper:]][[:lower:]]\+, [[:upper:]][[:lower:]]\+ [0-9]\+, [0-9]\{4\} [0-9]\+:[0-9]\{2\} [APM]*" "${!CPATH}"/.ad.txt`
		#  Extract "poster's" age
		AGE=`grep -oP "(?<=Poster's age: )[0-9]*" "${!CPATH}"/.ad.txt`
		
		#  Extract location information
		LOCATION=`pcregrep -oM "(?<=Location:)\W*?.*\W*?(?=</div)" "${!CPATH}"/.ad.txt`
		LOCATION=`clean_var "$LOCATION"`
	
		#  Extract Post ID to other postings
		ID=`grep -oP '(?<=Post ID: )[0-9]*' "${!CPATH}"/.ad.txt`
				
		#  Extract "Other Ads by this user"
		OTHERADS=`grep -v "http://"$2".backpage.com/"$URL"" <(grep -o "http://[a-z]\+\./backpage\.com/"$X"/[a-z\-]\+/[0-9]\+" "${!CPATH}"/.ad.txt)`
		#  Write content to file and download images
		if [ ! -d "${!CPATH}"/"$X"/"$ID" ]; then
			mkdir -p "${!CPATH}/"$X""/"$ID"/imgs
			CCPATH="${!CPATH}"/"$X"/"$ID"
		else
			mkdir -p "${!CPATH}"/"$X"/"$ID"\(`ls "${!CPATH}"/"$X" | grep -c "$ID"`\)
			CCPATH="${!CPATH}"/"$X"/"$ID"\(`ls "${!CPATH}"/"$X" | grep -c "$ID"`\)
			echo ID: "$ID" already exists and may be duplcated
		fi
		echo writing content for "$ID" to "$CCPATH"
		IPATH="$CCPATH"/imgs
		echo "$DATETIME" > "$CCPATH"/datetime.txt
		echo "$BODY"	 > "$CCPATH"/body.txt
		echo "$AGE"	 > "$CCPATH"/age.txt
		echo "$LOCATION" > "$CCPATH"/location.txt
		if [ ! -z "$OTHERADS" ]; then
			echo "$OTHERADS" > "$CCPATH"/otheradurls.txt
		fi

		IMGURL=`paste -sd, <(grep -Po '(?<=(img src="))[^ ]*(?=")' "${!CPATH}"/.ad.txt)`
		if [ ! -z "$IMGURL" ]; then
			cd "$IPATH"
			echo writing images for "$ID" to "$IPAH"
			curl -Os "{"$IMGURL"}"
			cd - > /dev/null
		fi
	
		#  Needs verification of success
		echo "\""$DATETIME"\",\""$BODY"\","$AGE",\""$LOCATION"\","$ID",0" >> ./content/posting_content_raw.csv
		BODY=;DATETIME=;AGE=;LOCATION=;ID=;OTHERADS=;IMGURL=;
	done < "${!UPATH}"/"$X"/"$FNAME"_NS.txt
}
if [ -z "$1" ]; then
	echo "Error: Posting category or categories not given without defulat"
	echo "Usage: $0 [category,category,...] [geography] [geography] ..."
	exit 1
fi

if [ -z "$2" ]; then
	echo "Error: Geography or georgraphies not given without default"
	echo "Usage: $0 [category,category,...] [geography] [geography] ..."
	exit 1 
fi
if [ ! -d ./content ]; then
	mkdir ./content
fi
if [ ! -e ./content/posting_content_raw.csv -o -s ./content/posting_content_raw.csv ]; then
	echo DATTIME,BODY,AGE,LOCATION,ID,ADTYPE > ./content/posting_content_raw.csv
fi
#  Select geographies.. not sure how to implement this yet

#  Scrape topics
until [ -z "$2" ]; do
	SUPATH=./URLs/"$2"/sponsored
	NSUPATH=./URLs/"$2"non-sponsored
	PPATH=./.pages/"$2"
	NSCPATH=./content/"$2"/non-sponsored
	SCPATH=./content/"$2"/sponsored
#  Download URLS and check remove URLS previously downloaded
#  Still need to implement for subsequent pages
	for X in `eval echo {$CATEGORY}`; do
		PAGE=1
		FNAME="$2"_"$X"_"$DATE"

		if [ ! -d "$NSUPATH"/"$X" ]; then
			mkdir -p "$NSUPATH"/"$X"
		fi
		if [ ! -d "$SUPATH"/"$X" ]; then
			mkdir -p "$SUPATH"/"$X"
		fi
		if [ ! -d "$PPATH"/"$X" ]; then
			mkdir -p "$PPATH"/"$X"
		fi
		if [ ! -d "$NSCPATH"/"$X" ]; then
			mkdir -p "$NSCPATH"/"$X"
		fi
		if [ ! -d "$SCPATH"/"$X" ]; then
			mkdir -p "$SCPATH"/"$X"
		fi

		until [ "$PAGE" -gt 444 ]; do
			echo downloading URLs for "$2"/"$X"/page"$PAGE"
   
			curl -s ""$2".backpage.com/"$X"/&page="$PAGE"" > "$PPATH"/"$X"/"$FNAME"_pg"$PAGE".html
   #  Test to see if the posting pages has ended and there are "No more matches."
			grep -q  "<div><b>No matches found.</b><br><br></div>" "$PPATH"/"$X"/"$FNAME"_pg"$PAGE".html
			if [ $? -eq 0 ]; then
				echo No more posting at page: "$PAGE"
				break
			fi
   #  Find every posting and sponsord URLs on currnet page
			grep -v "sponsorBoxPlusImages" "$PPATH"/"$X"/"$FNAME"_pg"$PAGE".html | grep -o /"$X"/[a-z\-]*/[0-9]* | sort | uniq > "$NSUPATH"/"$X"/."$FNAME"_NS.txt
			grep "sponsorBoxPlusImages" "$PPATH"/"$X"/"$FNAME"_pg"$PAGE".html | grep -o /"$X"/[a-z\-]*/[0-9]* | sort | uniq > "$SUPATH"/"$X"/."$FNAME"_S.txt

     			if [ `ls -A -1 "$NSUPATH"/"$X" | wc -l` -le 1 ]; then 
       				mv "$NSUPATH"/"$X"/."$FNAME"_NS.txt "$NSUPATH"/"$X"/"$FNAME"_NS.txt
     			else
       				sort -u "$NSUPATH"/"$X"/."$FNAME"_NS.txt `ls -A -d  "$NSUPATH"/"$X"/*` > "$NSUPATH"/"$X"/"$FNAME"_NS.txt
       				rm "$NSUPATH"/"$X"/."$FNAME"_NS.txt
     			fi
			if [ `ls -A -1 "$SUPATH"/"$X" | wc -l` -le 1 ]; then
				mv "$SUPATH"/"$X"/."$FNAME"_S.txt "$SUPATH"/"$X"/"$FNAME"_S.txt
			else
				sort -u "$SUPATH"/"$X"/."$FNAME"_S.txt `ls -A -d "$SUPATH"/"$X"/*` > "$SUPATH"/"$X"/"$FNAME"_S.txt
    				rm "$SUPATH"/"$X"/."$FNAME"_S.txt
			fi
			
    			((PAGE+=1))
		done 
#  Download and organize content--I need to functionalize this!
		echo downloading/extracting non-sponsored content for "$2"/"$X"
	download_content "NS"
	download_content "S"
	done
	shift
done

exit $?
