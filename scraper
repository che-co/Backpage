#!/bin/bash 
shopt -s extglob 

#  To-do: 
#  (1) Prevent URLS that have already been downloaded from being repeated

#  Author: Sergio "Checo" Gonzales
#  Date: 07-29-2016
#  This can be used to scrape the backpage.com

if [ -z "$1" ]; then
  echo "Error: Posting category or categories not given without defulat"
  echo "Usage: $0 [category,category,...] [geography] [geography] ..."
  exit 1
fi

if [ -z "$2" ]; then
  echo "Error: Geography or georgraphies not given without default"
  echo "Usage: $0 [category,category,...] [geography] [geography] ..."
  exit 1 
fi

CATEGORY="$1,"
DATE=`date +%d-%m-%Y`

#  Scrape topics
until [ -z "$2" ]; do
  for X in `eval echo {$CATEGORY}`; do
    export SUPATH=./URLs/"$2"/"$X"/sponsored
    export NSUPATH=./URLs/"$2"/"$X"/non-sponsored
    export PPATH=./.pages/"$2"/"$X"
    export NSCPATH=./content/"$2"/"$X"/non-sponsored
    export SCPATH=./content/"$2"/"$X"/sponsored
    PAGE=1
    FNAME="URLs_"$2"_"$X"_"$DATE""

    if [ ! -d "$NSUPATH" ]; then
      mkdir -p "$NSUPATH"
    fi
    if [ ! -d "$SUPATH" ]; then
      mkdir -p "$SUPATH"
    fi   
    if [ ! -d "$PPATH" ]; then
      mkdir -p "$PPATH"
    fi
    if [ ! -d "$NSCPATH" ]; then
      mkdir -p "$NSCPATH"
    fi
    if [ ! -d "$SCPATH" ]; then
      mkdir -p "$SCPATH"
    fi
    if [ ! -e ./content/"$2"/"$X"/posting_content_raw.csv -o -s ./content/"$2"/"$X"/posting_content_raw.csv ]; then
      echo DATTIME,BODY,AGE,LOCATION,ID,ADTYPE > ./content/"$2"/"$X"/posting_content_raw.csv
    fi

    until [ "$PAGE" -gt 444 ]; do
      echo downloading URLs for "$2"/"$X"/page"$PAGE"
      curl -s ""$2".backpage.com/"$X"/&page="$PAGE"" > "$PPATH"/"$FNAME"_pg"$PAGE".html
      grep -q  "<div><b>No matches found.</b><br><br></div>" "$PPATH"/"$FNAME"_pg"$PAGE".html
      if [ $? -eq 0 ]; then
	break
      fi
   #  Find every posting and sponsord URLs on currnet page
      grep -v "sponsorBoxPlusImages" "$PPATH"/"$FNAME"_pg"$PAGE".html | grep -o /"$X"/[a-z\-]*/[0-9]* | sort | uniq > "$NSUPATH"/."$FNAME"_NS.txt
      exNS=$?
      if [ $exNS -ne 0 ]; then
        echo warning: "$0": non-sponsored content not found on ""$2".backpage.com/"$X"/&page="$PAGE""
      fi
      grep "sponsorBoxPlusImages" "$PPATH"/"$FNAME"_pg"$PAGE".html | grep -o /"$X"/[a-z\-]*/[0-9]* | sort | uniq > "$SUPATH"/."$FNAME"_S.txt
      exS=$?
      if [ $exS -ne 0 ]; then 
        echo warning: "$0": sponsored content not found on ""$2".backpage.com/"$X"/&page="$PAGE""
      fi
      if [ $exNS -ne 0 -a $exS -ne 0 ]; then
         echo "$0": no content found on page ""$2".backpage.com/"$X"/&page="$PAGE"" \(maybe you have a typo\)
         exit 1
      fi
      if [ `ls -A -1 "$NSUPATH"/ | wc -l` -le 1 ]; then 
        mv "$NSUPATH"/."$FNAME"_NS.txt "$NSUPATH"/"$FNAME"_NS.txt
      else
        sort -u "$NSUPATH"/."$FNAME"_NS.txt `ls -A -d  "$NSUPATH"/*` > "$NSUPATH"/"$FNAME"_NS.txt
        rm "$NSUPATH"/."$FNAME"_NS.txt
      fi
      if [ `ls -A -1 "$SUPATH"/ | wc -l` -le 1 ]; then
	mv "$SUPATH"/."$FNAME"_S.txt "$SUPATH"/"$FNAME"_S.txt
      else
	sort -u "$SUPATH"/."$FNAME"_S.txt `ls -A -d "$SUPATH"/*` > "$SUPATH"/"$FNAME"_S.txt
	rm "$SUPATH"/."$FNAME"_S.txt
      fi
			
      ((PAGE+=1))
    done 
#  Download and organize content--I need to functionalize this!
    ./dlcnt "NS" "$2" "$X" "$FNAME"
    if [ $? -ne 0 ]; then
       exit $?
    fi
    ./dlcnt "S" "$2" "$X" "$FNAME"
    if [ $? -ne 0 ]; then
       exit $?
    fi 
  done
  shift
done

exit $?
