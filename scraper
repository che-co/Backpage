#!/bin/bash 
#  Author: Sergio "Checo" Gonzales
#  Date: 28-05-2016
#  This can be used to scrape the backpage.com

DATE=`date +%d-%m-%Y`
CATEGORY="$1"
NEWNAME="$CATEGORY"_"$2"_"$DATE"
NEWPOSTING="$NEWNAME".txt

#  Test for valid params and necessary directories.
#  If directories do not exist, create them.

if [ -z "$1" ]; then
 echo "Error: Posting category or categories not given without defulat"
 echo "Usage: $0 [category,category,...] [geography] [geography] ..."
 exit 1
fi

if [ -z "$2" ]; then
 echo "Error: Geography or georgraphies not given without default"
 echo "Usage: $0 [category,category,...] [geography] [geography] ..."
 exit 1 
fi

if [ ! -d ./URLS ]; then
 echo './URLS did not previously exist'
 mkdir ./URLS
fi

if [ ! -d .pages ]; then
 mkdir .pages
if

#  Select geographies.. not sure how to implement this yet

#  Scrape topics
until [ -z "$2" ]; do

 NEWNAME="$2"_"$CATEGORY"_"$DATE"
 NEWPOSTING="$NEWNAME".txt
##  May be useful if running twice in one day...
# if [ `ls ./URLS | grep -c "$NEWNAME"` -eq 0 ]; then
#  NEWPOSTING="$NEWNAME".txt
# else
#   NEWPOSTING="$NEWNAME"\(`ls ./URLS | grep -c "$NEWNAME"`\).txt
# fi
##

#  Download URLS and check remove URLS previously downloaded
#  Still need to implement for subsequent pages

 curl -s "$2".backpage.com/{"$CATEGORY",:}/"?=page""$page" > ./.pages/"$2"_{"$CATEGORY",:}_p"$page".html
 
 #  Find every posting URL on currnet page
 grep -o -e"http://[a-z]*.backpage.com/"{"$CATEGORY",:}"/[a-z\-]*/[0-9]*" ./.pages/"$2"_{"$CATEGORY",:}_p"$page".html | uniq > ./URLS/"$NEWNAME"_temp.txt

 #  Find all the paid ad URL on current page
 grep -o -e
 if [ `ls ./URLS/ | grep -c -e"$2"[a-z,_]*{"$CATEGORY",:}` -le 1 ]; then 
   sort -o ./URLS/"$NEWNAME"_temp.txt ./URLS/"$NEWNAME"_temp.txt
   mv ./URLS/"$NEWNAME"_temp.txt ./URLS/"$NEWPOSTING"
 else
   sort -u `ls ./URLS/"$2"* | grep -e{"$CATEGORY",:}` > ./URLS/"$NEWPOSTING"
   rm ./URLS/"$NEWNAME"_temp.txt
 fi

   echo "$2" completed
   shift
done

#  Record ads that are paid advertisment and remove duplicates
#+ from other URL lists



#  Download and organize content



exit $?
