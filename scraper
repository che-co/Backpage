#!/bin/bash  
#  Author: Sergio "Checo" Gonzales
#  Date: 07-06-2016
#  This can be used to scrape the backpage.com

DATE=`date +%d-%m-%Y`
CATEGORY="$1"
NEWNAME="$CATEGORY"_"$2"_"$DATE"
NEWPOSTING="$NEWNAME".txt
PAGE=1
#  Test for valid params and necessary directories.
#  If directories do not exist, create them.

if [ -z "$1" ]; then
 echo "Error: Posting category or categories not given without defulat"
 echo "Usage: $0 [category,category,...] [geography] [geography] ..."
 exit 1
fi

if [ -z "$2" ]; then
 echo "Error: Geography or georgraphies not given without default"
 echo "Usage: $0 [category,category,...] [geography] [geography] ..."
 exit 1 
fi

if [ ! -d ./URLS ]; then
 echo './URLS did not previously exist'
 mkdir ./URLS
fi

if [ ! -d .pages ]; then
 mkdir .pages
fi

#  Select geographies.. not sure how to implement this yet

#  Scrape topics
until [ -z "$2" ]; do

 NEWNAME="$2"_"$CATEGORY"_"$DATE"
 TEMPNAME=
 NEWPOSTING="$NEWNAME".txt
##  May be useful if running twice in one day...
# if [ `ls ./URLS | grep -c "$NEWNAME"` -eq 0 ]; then
#  NEWPOSTING="$NEWNAME".txt
# else
#   NEWPOSTING="$NEWNAME"\(`ls ./URLS | grep -c "$NEWNAME"`\).txt
# fi
##

#  Download URLS and check remove URLS previously downloaded
#  Still need to implement for subsequent pages
 for category in {"$CATEGORY",}; do
   curl -s "$2".backpage.com/"$category"/"&page=$PAGE" > ./.pages/"$2"_"$category"_p"$PAGE".html
 
 #  Find every posting URL on currnet page
   grep -o http://[a-z]*.backpage.com/"$category"/[a-z\-]*/[0-9]* ./.pages/"$2"_"$category"_p"$PAGE".html | uniq > ./URLS/"$NEWNAME"_temp.txt

 #  Find all the sponsored ad URLs on current page
   if [ `ls ./URLS/ | grep -c "$2"[a-z,_]*"$category"` -le 1 ]; then 
     sort -o ./URLS/"$NEWNAME"_temp.txt ./URLS/"$NEWNAME"_temp.txt
     mv ./URLS/"$NEWNAME"_temp.txt ./URLS/"$NEWPOSTING"
echo 1
   else
     sort -u `ls ./URLS/"$2"* | grep "$category"` > ./URLS/"$NEWPOSTING"
     rm ./URLS/"$NEWNAME"_temp.txt
echo 2
   fi
 done
 echo "$2", page "$PAGE" completed
# (($PAGE+=1)) 
 shift
done

#  Record ads that are paid advertisment and remove duplicates
#+ from other URL lists



#  Download and organize content



exit $?
