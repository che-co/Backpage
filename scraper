#!/bin/bash  

#  To-do: 
#  (1) Prevent URLS that have already been downloaded from being repeated

#  Author: Sergio "Checo" Gonzales
#  Date: 07-29-2016
#  This can be used to scrape the backpage.com

E_WRONG_ARG=85

if [ -z "$1" ]; then
  echo "Error: Posting category or categories not given without defulat"
  echo "Usage: $0 [category,category,...] [geography] [geography] ..."
  exit 1
fi

if [ -z "$2" ]; then
  echo "Error: Geography or georgraphies not given without default"
  echo "Usage: $0 [category,category,...] [geography] [geography] ..."
  exit 1 
fi

CATEGORY="$1,"
DATE=`date +%d%m%Y_%H%M%S`

#  Scrape topics
until [ -z "$2" ]; do
  for X in `eval echo {$CATEGORY}`; do
    export SUPATH=./URLs/"$2"/"$X"/sponsored
    export NSUPATH=./URLs/"$2"/"$X"/non-sponsored
    export PPATH=./.pages/"$2"/"$X"
    export NSCPATH=./content/"$2"/"$X"/non-sponsored
    export SCPATH=./content/"$2"/"$X"/sponsored
    PAGE=1
    FNAME="URLs_"$2"_"$X"_"$DATE""

    if [ ! -d "$NSUPATH" ]; then
      mkdir -p "$NSUPATH"
    fi
    if [ ! -d "$SUPATH" ]; then
      mkdir -p "$SUPATH"
    fi   
    if [ ! -d "$PPATH" ]; then
      mkdir -p "$PPATH"
    fi
    if [ ! -d "$NSCPATH" ]; then
      mkdir -p "$NSCPATH"
    fi
    if [ ! -d "$SCPATH" ]; then
      mkdir -p "$SCPATH"
    fi
    if [ ! -e ./content/"$2"/"$X"/posting_content_raw.csv -o ! -s ./content/"$2"/"$X"/posting_content_raw.csv ]; then
      echo DATTIME,BODY,AGE,LOCATION,ID,ADTYPE,URL > ./content/"$2"/"$X"/posting_content_raw.csv
    fi

    until [ "$PAGE" -gt 444 ]; do
      echo downloading URLs for "$2"/"$X"/page"$PAGE"
      curl -s ""$2".backpage.com/"$X"/&page="$PAGE"" > "$PPATH"/"$FNAME"_pg"$PAGE".html
      grep -q  "<div><b>No matches found.</b><br><br></div>" "$PPATH"/"$FNAME"_pg"$PAGE".html
      if [ $? -eq 0 ]; then
	break
      fi
   #  Find every posting and sponsord URLs on currnet page
      grep -v "sponsorBoxPlusImages" "$PPATH"/"$FNAME"_pg"$PAGE".html | grep -o /"$X"/[a-z\-]*/[0-9]* | sort | uniq > "$NSUPATH"/.NS.txt
      grep "sponsorBoxPlusImages" "$PPATH"/"$FNAME"_pg"$PAGE".html | grep -o /"$X"/[a-z\-]*/[0-9]* | sort | uniq > "$SUPATH"/.S.txt
      if [ ! -s "$SUPATH"/.S.txt -a ! -s "$NSUPATH"/.NS.txt ]; then 
         echo "$0": no relevent URLs and content found on page ""$2".backpage.com/"$X"/&page="$PAGE"" \(perhaps it\'s a typo\)
         exit $E_WRONG_ARG
      fi
      if [ ! -s "$SUPATH"/.S.txt ]; then 
        echo warning: "$0": sponsored content not found on ""$2".backpage.com/"$X"/&page="$PAGE"" >&2
      fi
      if [ ! -s "$NSUPATH"/.NS.txt ]; then
        echo warning: "$0": non-sponsored content not found on ""$2".backpage.com/"$X"/&page="$PAGE"" >&2
      fi

      if [ `ls -A -1 "$NSUPATH"/ | wc -l` -le 1 ]; then 
        mv "$NSUPATH"/.NS.txt "$NSUPATH"/"$FNAME"_NS.txt
      else
        sort -u "$NSUPATH"/.NS.txt `ls -A -d  "$NSUPATH"/*` > "$NSUPATH"/._NS.txt
        comm -23 "$NSUPATH"/._NS.txt "$NSUPATH"/.NS.txt > "$NSUPATH"/"$FNAME"_NS.txt 
        rm "$NSUPATH"/.NS.txt "$NSUPATH"/._NS.txt
      fi
      if [ `ls -A -1 "$SUPATH"/ | wc -l` -le 1 ]; then
	mv "$SUPATH"/.S.txt "$SUPATH"/"$FNAME"_S.txt
      else
	sort -u "$SUPATH"/.S.txt `ls -A -d "$SUPATH"/*` > "$SUPATH"/._S.txt
        comm -23 "$SUPATH"/._S.txt "$SUPATH"/.S.txt > "$SUPATH"/"$FNAME"_S.txt
        rm "$SUPATH"/.S.txt "$SUPATH"/._S.txt
      fi
			
      ((PAGE+=1))
    done 
    
    ./dlcnt "NS" "$2" "$X" "$FNAME"
    if [ $? -ne 0 -a $? -ne 44 ]; then
       exit $?
    fi
    ./dlcnt "S" "$2" "$X" "$FNAME"
    if [ $? -ne 0 -a $? -ne 44 ]; then
       exit $?
    fi 
  done
  shift
done

exit $?
